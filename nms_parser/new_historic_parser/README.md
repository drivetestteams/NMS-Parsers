# The new historic parser - parser_new_hisoric.py
# RESUME
This pyhton scripts collects, parses/tramsforms and inserts data into the NMS database. It runs inside the C:/TRISKELION_LOG_DATA/ folder, processing all its subfolders containing csv files with measurement data from systems. After successfully inserting data into the DB, the csv files are moved to C:/TRISKELION_LOG_DATA/IMPORTED folder. In case of failure, they are moved to C:/TRISKELION_LOG_DATA/PROBLEMATIC.

Every process included in this file is logged and exported into equivalent logfiles.

================================ FUNCTIONS ================================

# def csv_to_database(csv_path, engine)

Arguments: 
- csv_path: path where the processed csv file is located inside the individual system's folder
- engine: by default is mssql+pyodbc:///?odbc_connect=DRIVER={SQL Server Native Client 11.0};SERVER=SERVER NAME;DATABASE=DB NAME;UID=USERNAME;PWD=PASSWORD

** REPLACE SERVER NAME, DB NAME, USERNAME, PASSWORD with own configurations **

WHAT IT DOES:

Reads provided csv by MAIN, setting the delimiter to $, validating the [DATE] column, removing irrelevant columns like [Unnamed: ]. 
Next, it calls the function parse_dataframe_for_importing(df), where df is the dataframe argument containing the set of the csv file. 
After the success of parse_dataframe_for_importing(df) it inserts tne processed dataframe into the predefined DB table through df.to_sql function. df.to_sql uses the if_exists='append' argument, so that data is appended to proviously inserted ones and not overwritten. It also uses the index argument, to define each dataframe column's type and the engine argument, where the previous argument of the function is used.
In case of successful insertion into the DB, this function prints Imported along with the csv_path and retuns True. Else, it prints Not appended along with the csv_path and returns False.

** REPLACE FIRST AGRUMENT OF df.to_sql with own configuration ** 

# parse_dataframe_for_importing(df)

Arguments:
- df: dataframe provided by csv_to_database() containing already formed data from csv file. 
Drops invalid data, transforms Date/Time column and brings it to the appropriate format and Timezone and adds a new Column [SECTORID] generated by already existing [BESTS.CID].
Outputs necessary logging for the program monitoring. In case of failure prints out: Error processing DataFrame

================================ MAIN ================================
After creating the engine for the SQL insertion, it traverses all subdirectories and files of the local file repository (C:/TRISKELION_LOG_DATA/ by default) looking for new files.
Skips folders: IMPORTED, PROBLEMATIC and FASMETRICS, and starts looking only inside the OPERATORS folders and system subfolders.
Whenever it finds a csv file, it calls the csv_to_database(path,engine) to insert the file under examination into the db.
If the file is inserted succesfully, it is moved to the IMPORTED folder, otherwise it is moved to the PROBLEMATIC folder.